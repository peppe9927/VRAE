import torch
import torch.nn as nn
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence

class Supervisor(nn.Module):

    """ The supervisor network for the timeGAN model, it takes as input the embeddings 
    generated by the generator and returns a new set of embeddings that better follows
    the temporal dynamics. """

    def __init__(self, num_hidden, num_layers,max_seq,cell_type = 'GRU'):
        super(Supervisor,self).__init__()
        self.max_seq = max_seq
        if cell_type == 'LSTM':
            self.rnn = nn.LSTM(num_hidden, num_hidden, num_layers, batch_first=True)
        elif cell_type == 'GRU':
            self.rnn = nn.GRU(num_hidden, num_hidden, num_layers, batch_first=True)
        elif cell_type == 'RNN':
            self.rnn = nn.RNN(num_hidden, num_hidden, num_layers, batch_first=True)
        else:
            print('Invalid cell type, using LSTM')
            self.rnn = nn.GRU(num_hidden, num_hidden, num_layers, batch_first=True)

        self.fc = nn.Sequential(
            nn.Linear(num_hidden, num_hidden),
            nn.Sigmoid())
        
    def forward(self, x, T):
            """
            Parametri:
              x: (batch, seq_len, num_hidden)
              T : (Sequence Lengths)
            Restituisce:
              output: (batch, seq_len, num_hidden)
              hidden: (num_layers, batch, num_hidden)
            """
            T = T.cpu()
            packed = pack_padded_sequence(x, T, batch_first=True, enforce_sorted=False)
            output, hidden = self.rnn(packed)
            output, _ = pad_packed_sequence(output, batch_first=True, total_length=self.max_seq)
            S = self.fc(output)
            
            return S # retrn the refined embeddings