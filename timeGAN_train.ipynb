{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F  # Add this import for loss functions\n",
    "import time\n",
    "\n",
    "from VRAE import TimeGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def random_generator(batch_size, z_dim, T_mb, max_seq_len):\n",
    "    Z_mb = []\n",
    "    for i in range(batch_size):\n",
    "        # Inizializza un array di zeri con dimensione [max_seq_len, z_dim]\n",
    "        temp = np.zeros([max_seq_len, z_dim])\n",
    "        # Genera rumore per la parte attiva della sequenza\n",
    "        temp_Z = np.random.uniform(0., 1, [T_mb[i].item() if isinstance(T_mb[i], torch.Tensor) else T_mb[i], z_dim])\n",
    "        # Inserisci il rumore nel posto giusto\n",
    "        temp[:T_mb[i], :] = temp_Z\n",
    "        Z_mb.append(temp)\n",
    "    # Converte la lista in un array NumPy e poi in un tensore PyTorch\n",
    "    Z_mb = torch.tensor(np.array(Z_mb), dtype=torch.float32)\n",
    "    return Z_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.GRU) or isinstance(m, nn.LSTM) or isinstance(m, nn.RNN):\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "    elif isinstance(m, nn.Conv1d):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x122e8f0d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_seed = 42\n",
    "gen_lr = 0.001\n",
    "dis_lr = 0.001\n",
    "embedder_lr = 0.001\n",
    "supervise_lr = 0.001\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 64\n",
    "LATENT_DIM = 100\n",
    "gamma = 0.1\n",
    "seq_len = 50\n",
    "input_dim = 5\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = TimeGAN(\n",
    "    input_dim=input_dim,       # input dimension\n",
    "    hidden_dim=100,            # embedding dimension\n",
    "    num_layers=1,              # number of layers\n",
    "    cell_type='GRU'\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizers (one per parameter group)\n",
    "optimizer_E = optim.Adam(\n",
    "    list(model.embedder.parameters()) + list(model.recovery.parameters()),\n",
    "    lr=embedder_lr\n",
    ")\n",
    "optimizer_G = optim.Adam(\n",
    "    list(model.generator.parameters()) + list(model.supervisor.parameters()),\n",
    "    lr=gen_lr\n",
    ")\n",
    "optimizer_D = optim.Adam(model.discriminator.parameters(), lr=dis_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss functions\n",
    "mse_loss = nn.MSELoss()\n",
    "bce_loss = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of loaded file: <class 'dict'>\n",
      "Keys: dict_keys(['data', 'lengths', 'max_length'])\n",
      "data shape: torch.Size([2175, 3110, 5])\n",
      "lengths shape: torch.Size([2175])\n",
      "max_length type: <class 'int'>\n",
      "Batch 0:\n",
      "Real samples shape: torch.Size([64, 3110, 5])\n",
      "Sequence lengths: tensor([1134,  866, 1815, 1170, 1805, 1058, 1739, 1953, 2112, 2425, 1748, 1353,\n",
      "        2041, 1874, 2002, 1838, 1235, 2123, 2133, 1572, 1844, 2322, 1340, 2021,\n",
      "        1899, 1073, 1251, 1478, 2185, 2245, 1146, 2080, 1492,  737, 2140, 1444,\n",
      "        1212,  756, 1967, 1109, 1848, 1946, 1139, 1100, 1969, 1939, 1074, 1415,\n",
      "        1446, 1995, 1694, 1869, 1403, 2521, 1255, 1410, 1107, 1752, 2091, 1470,\n",
      "        1156, 1860, 2535, 1129])\n",
      "Batch 1:\n",
      "Real samples shape: torch.Size([64, 3110, 5])\n",
      "Sequence lengths: tensor([1892, 1665,  788, 1879, 1812, 2438, 1778, 1196, 1955, 2114, 1829, 1048,\n",
      "        1759, 1755, 2458, 1740, 1493, 1829, 1845, 1915, 1965, 1478, 1839, 2130,\n",
      "        1894, 1191,  750, 2074, 1809, 1086, 1996, 2223, 1773, 2712, 1856, 2048,\n",
      "        1486, 1350, 1069, 2146, 1922, 1065, 2021, 2385, 2051, 2280, 2275, 1945,\n",
      "        2171, 1847, 1736, 1953, 2192, 2046, 1126, 1492, 2164, 1535, 1477, 1039,\n",
      "        2344,  742, 1454, 1197])\n",
      "Batch 2:\n",
      "Real samples shape: torch.Size([64, 3110, 5])\n",
      "Sequence lengths: tensor([1863, 1597, 1607, 1429, 2101, 1274, 2360, 1974, 2019, 2524, 1839, 1135,\n",
      "        1508, 1945, 1833, 1903, 1162, 1142, 3110, 1279, 2464, 2068, 2018, 1868,\n",
      "        1511, 2048, 1914, 1069, 1093,  812, 2247, 2252, 1170, 1779,  808, 1867,\n",
      "        2188,  933, 2326, 1858, 2218, 2491, 1448, 1627, 2270, 2276, 1619, 1372,\n",
      "        1958, 2776, 2954, 2291, 1702, 1468,  774, 2391, 1188, 2282, 1721, 2251,\n",
      "        1776, 1960, 1747,  868])\n",
      "Batch 3:\n",
      "Real samples shape: torch.Size([64, 3110, 5])\n",
      "Sequence lengths: tensor([ 752, 1936, 3006, 2614, 1406, 2001, 2005, 2220, 2197, 2015, 1122,  679,\n",
      "        2117, 1994, 2000, 1415, 2081, 1348, 1188, 2034, 1103, 1205, 1744, 2139,\n",
      "        1211, 2201, 1816, 1074, 1849, 1610, 2062, 1990, 1377, 1553, 2094, 1970,\n",
      "        2045, 1861, 1446, 2679, 2363, 1382, 2451, 1790, 2014, 1799, 1888, 1829,\n",
      "         874, 1848, 1540, 1489, 1642, 2000, 1898, 1383, 1839, 2106,  763, 2148,\n",
      "        2309, 1853, 2296, 1410])\n",
      "Batch 4:\n",
      "Real samples shape: torch.Size([64, 3110, 5])\n",
      "Sequence lengths: tensor([2039, 2323, 2493, 1328, 1971, 2804, 1618, 1753, 1382, 1234, 1770, 1079,\n",
      "        1131, 2129, 1822, 2430,  759, 1740, 2111, 2130, 1952, 1135, 1119, 1096,\n",
      "        1782, 1444,  781, 2314, 1905, 1149, 1024, 2091, 2352, 1985, 1979, 1846,\n",
      "        1979, 2109,  805, 1715, 2076, 2176, 2474, 2039, 1943, 2431, 1870, 1593,\n",
      "        1741, 2380, 1280, 1346, 2287, 1998, 2127, 1384, 1996, 1086, 1609, 2700,\n",
      "        1170, 1740, 1755, 2065])\n",
      "Batch 5:\n",
      "Real samples shape: torch.Size([64, 3110, 5])\n",
      "Sequence lengths: tensor([2102, 2380,  810, 2128, 2396, 1955, 1750, 2177, 1768, 2230, 1908, 1384,\n",
      "        1122, 1024, 2033, 1535, 2186, 1371, 1923,  851,  984, 1217, 1579, 2377,\n",
      "        2093, 1785,  721, 1644, 2008, 1169, 1691,  791, 1709, 1537, 2089, 1877,\n",
      "        1756, 2177, 2068, 1458, 1975, 2538, 1307, 1864, 2218, 1275, 2253, 1508,\n",
      "        1778, 2126, 2056, 1778, 1478, 2172, 1117, 2071, 1628, 1980, 1787,  754,\n",
      "        2212, 2245, 2063, 2314])\n",
      "Batch 6:\n",
      "Real samples shape: torch.Size([64, 3110, 5])\n",
      "Sequence lengths: tensor([2109, 2363, 1692,  851,  991, 2069, 1046, 1266, 1626, 1024, 1243, 1009,\n",
      "        2194, 1990, 2450, 1558, 2090, 1620, 2510, 2341, 1730, 1743, 1934, 2092,\n",
      "        1483, 1793, 1020, 1968, 1375, 1166,  876, 2025, 1765, 1731, 1697, 1873,\n",
      "        2261, 1953, 1240, 2143, 1452, 1415, 1496, 1762, 1944, 1885, 1909, 1617,\n",
      "        1074, 1369, 1857, 1883, 1881, 2650, 1976, 1132, 1800, 1866, 1417, 1125,\n",
      "        2032, 1805, 1778, 1811])\n",
      "Batch 7:\n",
      "Real samples shape: torch.Size([64, 3110, 5])\n",
      "Sequence lengths: tensor([1990, 1198, 2227, 1267, 2094, 2196, 1294, 2038, 2465, 1946, 1852,  718,\n",
      "        1828, 2167, 1049, 2221, 1834, 1822, 1446, 1189, 2483, 2311, 1956, 2185,\n",
      "        1836, 1448, 1058, 2143, 1628, 2230, 1656, 1943, 2160, 2156, 2190, 1531,\n",
      "        2094, 2099, 2236, 2559, 1651, 1450, 2642, 1817, 2123, 2195, 1153, 2322,\n",
      "        1909, 2006, 2220, 2571, 1718, 2095, 2069, 2094, 1864, 1253, 2002, 2400,\n",
      "        1171, 1779, 1688, 2023])\n",
      "Batch 8:\n",
      "Real samples shape: torch.Size([64, 3110, 5])\n",
      "Sequence lengths: tensor([2498, 1721, 1567, 1399, 2324, 2263, 2015, 1647, 1841, 2365, 1382, 1834,\n",
      "        2124, 1129, 1561, 1101, 1883, 2156, 2125, 1621, 1954, 2293, 1910,  780,\n",
      "        2323, 1851, 1659, 2524, 1583, 2254, 2303, 1855, 2019, 1441, 1672, 2122,\n",
      "        1165, 1915, 1668, 1913, 2106, 1754, 1644, 1823, 2269, 2574, 1883, 1834,\n",
      "        1950, 1706, 1020, 2365, 2039, 1745, 1555, 1954, 2026, 2265, 2037, 1829,\n",
      "        1839, 2150, 1876,  839])\n",
      "Batch 9:\n",
      "Real samples shape: torch.Size([64, 3110, 5])\n",
      "Sequence lengths: tensor([1840, 1954, 2088, 1058, 1112, 1048, 2213, 1644, 1905, 2507, 2023, 1520,\n",
      "        2362, 1176, 2214, 1675, 1040, 2678, 2106, 1825, 2784, 2280, 2218, 1107,\n",
      "        1654, 2436, 1195, 2367, 2066, 1144, 2078, 1538, 1971, 1469, 1723, 1663,\n",
      "        1770, 2729, 1328, 2024, 2215,  717, 1770, 1043, 1831, 1965, 1840, 2464,\n",
      "        2239, 2446, 2198, 1033, 1815,  710, 1445, 2095, 2536, 1452, 2390, 1898,\n",
      "        1101, 2193,  772, 2047])\n",
      "Batch 10:\n",
      "Real samples shape: torch.Size([64, 3110, 5])\n",
      "Sequence lengths: tensor([1046, 1991, 1584, 1875,  804, 2192, 2206, 2059, 1626, 1120, 1165, 1521,\n",
      "        1952, 1889, 1928, 2312, 1268,  853, 1561, 1657, 2156, 1816, 1966, 1285,\n",
      "        2260, 1843, 1459, 1948, 1714, 1422, 2632, 2108, 1497, 1832,  833, 2000,\n",
      "        2230, 1133, 1880, 1773,  747, 1699, 1676, 1969,  929, 1895, 1812, 2210,\n",
      "        2358, 1940, 1809,  810, 1693, 1105, 1878, 1868, 1566, 2059, 2102, 1994,\n",
      "        2290, 1390, 1999, 1534])\n",
      "Batch 11:\n",
      "Real samples shape: torch.Size([64, 3110, 5])\n",
      "Sequence lengths: tensor([1680,  778,  871, 1092, 1933, 1195, 1906, 2255, 2193, 2147, 1466, 2332,\n",
      "        1706, 2124, 1726, 1995, 1437, 1925, 1883, 1322, 2100, 2370, 1103, 1633,\n",
      "        2024,  776, 1681, 2134, 1218,  762, 1097, 1437, 2006, 1105, 2031, 1963,\n",
      "        1543, 2404, 1070, 1665, 1741, 2644, 1419, 1946, 1805,  991, 1488, 2209,\n",
      "        2099, 2309, 1690, 2137, 2038, 1450, 1560, 1498, 2107, 2201, 1309, 1832,\n",
      "        1386, 1420, 1671, 1251])\n",
      "Batch 12:\n",
      "Real samples shape: torch.Size([64, 3110, 5])\n",
      "Sequence lengths: tensor([2079,  767, 1336, 2174, 1593, 2160, 2245, 1896, 1740, 1933, 1934, 1511,\n",
      "        1854, 1703, 2050, 2143, 2098, 1931, 1862, 1372, 1908, 1404, 1521, 1964,\n",
      "        2076, 1703, 1876, 1898, 1204, 1009, 1114, 2931, 2648, 1999, 1411, 2435,\n",
      "        1858, 2038, 1326, 1501, 2383, 1492, 2089, 2449, 1687, 1767, 1518, 1824,\n",
      "        1979, 1898, 1114, 2388, 1498, 1859,  985, 2339, 1799, 2680,  908, 1738,\n",
      "        1044, 2277, 1445, 1273])\n",
      "Batch 13:\n",
      "Real samples shape: torch.Size([64, 3110, 5])\n",
      "Sequence lengths: tensor([2122, 2207, 1451, 1285, 1389, 1846, 1345, 1484, 1888, 2094, 2265, 2042,\n",
      "        2022, 2004, 1169, 1667, 2535, 1882, 2155, 1516, 1784, 1435, 1261, 1312,\n",
      "        1126, 1846, 1683, 1795, 2095, 1890, 1437, 2247, 1120, 2428, 1476, 2383,\n",
      "        2003, 1821, 2030, 1045,  810, 1162, 1268, 1468, 1706, 1893, 2460, 1571,\n",
      "        1560, 1684, 2243, 2243, 1893, 1924, 1831, 2859, 1484, 1001, 2080, 1753,\n",
      "        2331, 1862, 1640, 1448])\n",
      "Batch 14:\n",
      "Real samples shape: torch.Size([64, 3110, 5])\n",
      "Sequence lengths: tensor([1030, 2324,  785, 1792, 2157, 2217, 2224, 1636, 2472, 1521, 2436,  728,\n",
      "         776, 2111, 1878, 1743, 1461, 1260, 1558, 1409, 1733, 1928,  802,  893,\n",
      "        2095, 1207, 1609, 1445, 2035, 2197, 2077, 2451, 2291, 2161, 1134, 2355,\n",
      "        1167, 2264, 2175, 2430, 1961, 1965, 1768, 1792, 1473, 1109, 1458, 2090,\n",
      "        1109,  825, 1139, 1325, 1766, 1323, 2669, 2024, 1805, 1985, 2215, 2292,\n",
      "        2261, 1843, 1702, 1485])\n",
      "Batch 15:\n",
      "Real samples shape: torch.Size([64, 3110, 5])\n",
      "Sequence lengths: tensor([2034, 2190, 2885, 1121, 1921, 1520, 2157, 2283, 1855, 2243, 1249,  798,\n",
      "        1134,  821, 1510, 2020, 2292, 1944, 1666, 2105, 2290, 2349, 2306, 1782,\n",
      "        2003, 1832, 2610, 1305, 1884, 2309,  941, 1985, 2005, 1225, 1776, 2220,\n",
      "        2411, 1694,  908, 1158, 1486, 1016, 1676, 2057, 2030, 1965, 1187, 2340,\n",
      "        1757, 1511, 1455, 1893,  737, 2050, 2001, 2047, 2273, 1115, 1820, 2483,\n",
      "        1307, 2290, 2022, 2273])\n",
      "Batch 16:\n",
      "Real samples shape: torch.Size([64, 3110, 5])\n",
      "Sequence lengths: tensor([1591, 1017, 2285, 2073, 2277,  987, 1449, 1501,  749,  827, 2125, 1289,\n",
      "        1841, 1823, 1548, 1818, 1826, 1368, 1683, 1025, 1399, 2140, 1185, 1951,\n",
      "        2875,  748, 1738, 2078, 1696, 2029, 1800, 1029, 1417, 1559, 1886,  740,\n",
      "        1330, 2284, 1481, 1956, 2416, 2202, 1409, 1668, 2261, 2077, 1999, 2225,\n",
      "        1139, 1695, 1681, 2469, 2296, 1244, 1801, 1002, 2099, 2166, 1843,  941,\n",
      "        1128,  742,  765, 1538])\n",
      "Batch 17:\n",
      "Real samples shape: torch.Size([64, 3110, 5])\n",
      "Sequence lengths: tensor([1815, 2432, 2386,  704, 2258, 1187, 1254, 1734, 1993, 2231, 1946, 1943,\n",
      "        1216, 1988, 1833, 2063, 1707, 2474, 1766, 2117, 1158, 2212, 1989, 1895,\n",
      "        2193, 1716, 1983, 2052, 1006, 1680, 1447, 1304, 1731, 1680, 2202, 2058,\n",
      "        1118, 2517, 1537, 1871,  841, 1132,  733, 1489, 2136, 1475, 1410, 2258,\n",
      "        1670, 1630, 1973, 1752, 1450, 1715, 1251,  880, 2320, 1882, 2321, 1558,\n",
      "         902, 1213, 2366, 1733])\n",
      "Batch 18:\n",
      "Real samples shape: torch.Size([64, 3110, 5])\n",
      "Sequence lengths: tensor([2220, 1432, 2316, 1911, 1858, 1474, 1089, 2084, 1338, 2391, 1835, 2039,\n",
      "        1345, 1051, 1328, 1789, 1355, 1556, 2328, 1869, 1634,  791, 2627, 2176,\n",
      "        2048, 2500, 2646, 2117, 2219, 2294, 2240, 1443, 1161, 1365, 1364, 2098,\n",
      "        1103, 2413, 2463, 1609, 2123, 2681, 2014, 2098, 1440, 1160, 2056, 1828,\n",
      "        2106, 1459, 1285, 1857, 1966, 1900, 1710, 1600, 2065, 1820, 2181, 2031,\n",
      "        2158, 2255, 1128, 2179])\n",
      "Batch 19:\n",
      "Real samples shape: torch.Size([64, 3110, 5])\n",
      "Sequence lengths: tensor([ 816, 2159, 1691, 1929, 1571, 1589, 1801, 2262, 2741, 2128, 2006, 2741,\n",
      "        1016,  770, 1950, 2418, 1218,  765, 1824, 1538, 1435, 2377,  897, 1147,\n",
      "        1114, 2055, 2510, 2245,  745, 2237, 1996, 1987, 1112, 1482, 2343, 1310,\n",
      "        1528, 1876, 1865, 1135, 1832, 2502, 1606, 2139, 1663, 2321, 2194, 2084,\n",
      "        1954, 2298, 1633, 2487, 1837, 1901, 1739, 1457, 2662, 2200, 2075, 1592,\n",
      "        2601, 1844, 1391, 1992])\n",
      "Batch 20:\n",
      "Real samples shape: torch.Size([64, 3110, 5])\n",
      "Sequence lengths: tensor([1094, 2142, 1875, 1941, 1260, 2148, 1949, 1370, 1845, 1502, 2137, 2497,\n",
      "        1784, 1791, 1886, 2363, 1716, 1459, 1961, 1260, 1554, 1350, 2379, 1730,\n",
      "        1318, 2068,  758, 1366, 2057, 1963, 1982, 1910,  903, 1994, 2138, 1669,\n",
      "         748, 1949, 1816, 1940, 1440, 2539, 2325, 1919, 1891, 1346, 2017, 1162,\n",
      "        1454, 1805, 2085, 2244,  747, 1235, 1943, 1041, 2083, 1758, 1938, 1865,\n",
      "        2288, 2036, 2149, 2047])\n",
      "Batch 21:\n",
      "Real samples shape: torch.Size([64, 3110, 5])\n",
      "Sequence lengths: tensor([2381, 1969, 2014,  949, 1605, 1635, 1407, 1470, 1122, 1831, 2366, 2106,\n",
      "        2036, 1518, 1343, 1950, 1104, 2206, 2076, 1812, 1608, 2185, 2124, 1320,\n",
      "        1562, 1440, 1287, 2179, 2155, 2140,  883, 1454, 1688, 1498, 1959, 1726,\n",
      "        2541, 1971, 1129, 1150, 1836, 2661, 1218, 1097, 2259, 1485, 1537,  880,\n",
      "        2267, 2423, 2040, 2067, 2226, 1180, 2003, 1915, 2359, 1682, 1737, 1840,\n",
      "        1694, 1501, 2332, 1100])\n",
      "Batch 22:\n",
      "Real samples shape: torch.Size([64, 3110, 5])\n",
      "Sequence lengths: tensor([ 784, 2057, 1780, 2294, 1718, 2003, 2252, 2311, 1813, 2631, 1625, 1908,\n",
      "        1733, 1740, 1419, 1487, 1695, 1170, 1831, 1909, 1969, 1397, 2596, 1123,\n",
      "        1595, 1895, 2294, 1134, 1916, 1481, 1919, 2027, 1602, 1384, 2238, 2236,\n",
      "        1877, 2149, 1672, 2072, 1081, 1967, 1417, 2157, 2586, 2035, 1502, 1458,\n",
      "        2219,  801, 1350, 2590, 1733, 1994, 1774, 1011, 1995, 1816, 1167, 1420,\n",
      "        1844,  808, 1443, 1891])\n",
      "Batch 23:\n",
      "Real samples shape: torch.Size([64, 3110, 5])\n",
      "Sequence lengths: tensor([2345, 1835, 2256, 1651, 1775, 1284, 1798, 1995, 2065, 2307, 2013, 1506,\n",
      "        2235, 1454, 1713, 2478, 2255, 1801, 1891, 2746, 1704, 2201,  841, 2106,\n",
      "        2084, 2044, 1772, 1863, 2052, 2366, 1218, 2366, 1075, 1993, 2005, 2232,\n",
      "        2436, 2573,  850, 2120, 1498, 1351, 1695,  714, 2484, 1168, 1800, 1444,\n",
      "         738,  871, 1294, 1168, 2384, 1836, 1306, 2121, 1099, 2144, 2469, 1941,\n",
      "        1543, 1735, 2132, 1980])\n",
      "Batch 24:\n",
      "Real samples shape: torch.Size([64, 3110, 5])\n",
      "Sequence lengths: tensor([2188, 1684, 1997, 1871, 1827, 1466, 1734, 1981, 1060, 1823, 2695, 1093,\n",
      "        2291, 2289, 1422, 1824, 1059, 1805, 1865, 2391, 1393,  865, 1431, 2225,\n",
      "        1936, 1929, 1939, 2196, 2064, 1988, 2001, 2673, 1795, 1466, 1933, 1632,\n",
      "        1881,  719, 1540, 2110, 2109, 1452, 2314, 1137,  900, 2304, 2354, 1206,\n",
      "        2188, 1730,  761, 3083, 1878, 1039, 2091, 1055, 2236, 1806, 1162, 1048,\n",
      "        1129, 2252, 2155, 1695])\n",
      "Batch 25:\n",
      "Real samples shape: torch.Size([64, 3110, 5])\n",
      "Sequence lengths: tensor([2235, 1841, 2308, 1962, 1374, 2056, 1886, 1533, 1830, 1750, 1171, 1131,\n",
      "        2635, 1463, 2686, 2097, 1288, 2325, 1464, 2285, 1997, 2653, 2408, 1587,\n",
      "        2404,  746, 2340, 1500, 1363, 2265, 2182, 1636, 2302, 1672, 2846, 1099,\n",
      "        2234, 2336, 2095, 1553, 1408, 1167, 1260, 2113, 1825, 2542, 2158, 1203,\n",
      "        2064, 2122, 2280, 2101, 2107, 1947, 1543, 1779, 1919, 2261, 2234, 1750,\n",
      "         767, 1406, 1937, 1881])\n",
      "Batch 26:\n",
      "Real samples shape: torch.Size([64, 3110, 5])\n",
      "Sequence lengths: tensor([2579, 2297, 1568, 1716, 1833, 1990, 2657, 1786, 2208, 1512, 1181, 2354,\n",
      "        2482, 1445, 1785, 1175, 1383, 1573, 1633, 2415, 1939, 2296, 1320, 2470,\n",
      "        2176, 1545, 1931, 2061, 2164, 1847, 1133, 1053, 1374, 1777,  837, 2049,\n",
      "        2199, 1696, 1213,  760, 2059, 1375, 2177, 1780, 2391, 2276, 2265, 1845,\n",
      "        1462, 2484, 2251, 1909, 1533, 2758,  764, 1621, 1557, 2500,  891, 1551,\n",
      "        1986, 2212, 2186, 1185])\n",
      "Batch 27:\n",
      "Real samples shape: torch.Size([64, 3110, 5])\n",
      "Sequence lengths: tensor([2281, 1224, 2273, 1669, 2238, 1146, 2052,  764,  923, 1342, 1838, 1819,\n",
      "        2364, 1888, 1118, 1959, 1638, 1197, 1848, 1104, 1887,  733, 2074, 1483,\n",
      "        1349, 1825, 2050, 1938, 1889, 2378,  913, 1305, 1778, 2365, 1946, 2180,\n",
      "        2359, 2039, 1001, 1698, 2139, 1393, 1244, 1488, 1435, 1349, 1561, 1779,\n",
      "        1915, 1164, 2246, 2691, 2420, 1556, 2475, 1586, 2003, 1924, 1109, 2026,\n",
      "         840, 2232, 1451, 1791])\n",
      "Batch 28:\n",
      "Real samples shape: torch.Size([64, 3110, 5])\n",
      "Sequence lengths: tensor([2274, 1635, 2418, 2476, 2544, 1295, 1135, 1292, 1382,  910, 2193, 1431,\n",
      "        1836, 2131, 2136, 1765, 2027, 1959, 1687,  935, 1554,  742, 1115, 1184,\n",
      "        1933, 1116, 1831, 2083, 1802, 2014, 1705, 1013, 1984, 2094, 1938, 2013,\n",
      "        1694, 1423, 1745,  892, 1978, 1845, 1231, 1943, 1792, 2048, 1686, 1803,\n",
      "        2288, 2289, 1115, 1998, 2093, 1710, 1455, 2013, 1171, 1319, 2047, 1953,\n",
      "        1747, 2226,  956, 1995])\n",
      "Batch 29:\n",
      "Real samples shape: torch.Size([64, 3110, 5])\n",
      "Sequence lengths: tensor([1920, 1935, 2219,  863, 2066, 1236, 2476, 2168, 2045, 1051, 1309, 2057,\n",
      "        2653, 2514, 1932, 1621, 1481, 2023, 1825, 1616, 1924, 2532, 2233, 1164,\n",
      "        1917, 2570, 2052, 1943, 2020, 2154, 1327, 2214, 1638, 2085, 1951, 1891,\n",
      "        2137, 2374, 2006, 2003, 1789, 2191, 1635, 2003, 1906, 1743, 1217, 1208,\n",
      "        1841, 2296, 1052, 2166,  958, 2125, 1889, 2523, 1834, 2494, 2446, 1462,\n",
      "        1462, 1582, 1878, 2531])\n",
      "Batch 30:\n",
      "Real samples shape: torch.Size([64, 3110, 5])\n",
      "Sequence lengths: tensor([1616, 1873,  839, 1076, 2030, 1944, 2135, 1915, 1888,  761,  822, 1559,\n",
      "        2031, 2083, 1309, 1854, 1458, 1485, 1141, 1870,  805, 1541, 2093, 1553,\n",
      "        1480, 2161, 1125, 2209, 1876, 1120, 2096, 1139, 2135, 1707, 1574, 1843,\n",
      "        1092, 1918, 1543, 1923, 2086, 2038, 2255, 2581, 2471, 1658, 1881, 1974,\n",
      "        1234, 1946, 1401, 1990, 1875, 1062, 1332, 1964, 1667, 2630, 1943, 1776,\n",
      "         850, 2191, 1567, 1816])\n",
      "Batch 31:\n",
      "Real samples shape: torch.Size([64, 3110, 5])\n",
      "Sequence lengths: tensor([1755, 1842, 1594, 1947, 2449, 1533,  890, 2187, 1863, 1836, 1631, 1996,\n",
      "        1545, 1469, 1103, 1986, 1969, 1771, 1050,  733, 1863, 1772, 2130, 1881,\n",
      "        2850, 2120, 2143, 1438, 2284, 2643, 2085, 1701, 1497, 1947, 1852, 1775,\n",
      "        2340, 2390, 1094, 1769, 2013, 1858, 1484, 2241, 1931, 2299, 2716, 2670,\n",
      "        1550, 1782,  800, 1831, 2075, 1733,  729, 1017, 2259, 2107,  930, 1681,\n",
      "        1718, 2274, 2396, 1553])\n",
      "Batch 32:\n",
      "Real samples shape: torch.Size([64, 3110, 5])\n",
      "Sequence lengths: tensor([2437, 1084, 2735, 1317, 2007, 1877, 1852, 1795, 2284, 2303,  726, 1469,\n",
      "        1764, 1585, 1407, 1890, 2541, 1091, 1476, 1928, 1732, 2166, 2022, 1650,\n",
      "        1411, 1249, 1472, 1453, 2005, 2175, 2183, 1935, 2437, 1471, 1834, 1835,\n",
      "        2466, 1846, 2524, 1356, 1805, 2360, 1551, 2323,  798, 1029,  747, 1332,\n",
      "        2411, 1722, 1872, 2579, 1775, 1909, 2230, 1938, 2084, 1204, 1889, 2181,\n",
      "        2366, 1021, 1678, 1502])\n",
      "Batch 33:\n",
      "Real samples shape: torch.Size([63, 3110, 5])\n",
      "Sequence lengths: tensor([2130, 1487, 1941, 2327, 1022, 1243, 1288, 2223, 1305, 2014, 1123, 2157,\n",
      "        2101, 2502, 2596, 1112, 2002, 1141, 2536,  969, 1975, 1908, 1789, 1664,\n",
      "        1219, 2092, 2491, 2314, 1688, 1918, 2036, 2124, 1987, 2097, 1941, 1863,\n",
      "        1959, 2689, 1231, 1493,  820, 1713, 2186, 1673, 1735, 1071, 1828, 2030,\n",
      "        1599, 1786, 1836, 1069, 1244, 1948, 1034, 2181, 1613, 1066, 1462, 1002,\n",
      "         818, 1757, 2088])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Load the trajectory dataset\n",
    "trajectory_dataset = torch.load(\"./VRAE/trajectory_dataset.pt\")\n",
    "\n",
    "# Print basic information about the dataset\n",
    "print(f\"Type of loaded file: {type(trajectory_dataset)}\")\n",
    "\n",
    "\n",
    "\n",
    "# If it's a dictionary, print the keys\n",
    "if isinstance(trajectory_dataset, dict):\n",
    "    print(f\"Keys: {trajectory_dataset.keys()}\")\n",
    "    # Print shapes of items\n",
    "    for key, value in trajectory_dataset.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            print(f\"{key} shape: {value.shape}\")\n",
    "        elif hasattr(value, '__len__'):\n",
    "            print(f\"{key} length: {len(value)}\")\n",
    "        else:\n",
    "            print(f\"{key} type: {type(value)}\")\n",
    "# If it's a tensor, print its shape\n",
    "elif isinstance(trajectory_dataset, torch.Tensor):\n",
    "    print(f\"Tensor shape: {trajectory_dataset.shape}\")\n",
    "# If it's another container type, print its length\n",
    "elif hasattr(trajectory_dataset, '__len__'):\n",
    "    print(f\"Dataset length: {len(trajectory_dataset)}\")\n",
    "\n",
    "max_seq_len = trajectory_dataset['max_length']\n",
    "\n",
    "dataset = TensorDataset(trajectory_dataset['data'], trajectory_dataset['lengths'])\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "for i, (real_samples, seq_lens) in enumerate(train_loader):\n",
    "    print(f\"Batch {i}:\")\n",
    "    print(f\"Real samples shape: {real_samples.shape}\")\n",
    "    print(f\"Sequence lengths: {seq_lens}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (rnn): GRU(100, 100, batch_first=True)\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_layers, max_seq = 50, cell_type = 'GRU'):\n",
    "        super(Generator,self).__init__()\n",
    "        self.max_seq = max_seq\n",
    "        if cell_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        elif cell_type == 'GRU':\n",
    "            self.rnn = nn.GRU(hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        elif cell_type == 'RNN':\n",
    "            self.rnn = nn.RNN(hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        else:\n",
    "            print('Invalid cell type, using LSTM')\n",
    "            self.rnn = nn.GRU(hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, x, T):\n",
    "        \n",
    "        packed = pack_padded_sequence(x, T, batch_first=True, enforce_sorted=False)\n",
    "        output, hidden = self.rnn(packed)\n",
    "        output, _ = pad_packed_sequence(output, batch_first=True,total_length=self.max_seq)\n",
    "        E = self.fc(output)\n",
    "\n",
    "        return E # return the generated data embeddings\n",
    "    \n",
    "\n",
    "hidden_dim_generator = 100\n",
    "num_layers_generator = 1\n",
    "cell_type_generator = 'GRU'\n",
    "generator = Generator(hidden_dim = hidden_dim_generator,\n",
    "                      num_layers = num_layers_generator,\n",
    "                      max_seq = max_seq_len,\n",
    "                      cell_type = cell_type_generator)\n",
    "generator.apply(weights_init)\n",
    "generator.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (rnn): GRU(100, 100, batch_first=True, bidirectional=True)\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=200, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_hidden, num_layers, max_seq, cell_type = 'GRU'):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.max_seq = max_seq\n",
    "        if cell_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(num_hidden, num_hidden, num_layers, batch_first=True, bidirectional=True)\n",
    "        elif cell_type == 'GRU':\n",
    "            self.rnn = nn.GRU(num_hidden, num_hidden, num_layers, batch_first=True, bidirectional=True)\n",
    "        elif cell_type == 'RNN':\n",
    "            self.rnn = nn.RNN(num_hidden, num_hidden, num_layers, batch_first=True, bidirectional=True)\n",
    "        else:\n",
    "            print('Invalid cell type, using default GRU')\n",
    "            self.rnn = nn.GRU(num_hidden, num_hidden, num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Output size is doubled due to bidirectional RNN\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(num_hidden*2, 1),\n",
    "            nn.Sigmoid())\n",
    "    \n",
    "    def forward(self, x, T):\n",
    "        \n",
    "        packed = pack_padded_sequence(x, T, batch_first=True, enforce_sorted=False)\n",
    "        output, hidden = self.rnn(packed)\n",
    "        output, _ = pad_packed_sequence(output, batch_first=True, total_length=self.max_seq)\n",
    "        Y_hat = self.fc(output)\n",
    "\n",
    "        return Y_hat\n",
    "    \n",
    "num_layers_discriminator = 1\n",
    "hidden_dim_discriminator = 100\n",
    "cell_type_discriminator = 'GRU'\n",
    "\n",
    "discriminator = Discriminator(num_hidden = hidden_dim_discriminator,\n",
    "                              num_layers = num_layers_discriminator,\n",
    "                                max_seq = max_seq_len,\n",
    "                              cell_type = cell_type_discriminator)\n",
    "discriminator.apply(weights_init)\n",
    "discriminator.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Supervisor(\n",
       "  (rnn): GRU(100, 100, batch_first=True)\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class Supervisor(nn.Module):\n",
    "\n",
    "    \"\"\" The supervisor network for the timeGAN model, it takes as input the embeddings \n",
    "    generated by the generator and returns a new set of embeddings that better follows\n",
    "    the temporal dynamics. \"\"\"\n",
    "\n",
    "    def __init__(self, num_hidden, num_layers,max_seq,cell_type = 'GRU'):\n",
    "        super(Supervisor,self).__init__()\n",
    "        self.max_seq = max_seq\n",
    "        if cell_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(num_hidden, num_hidden, num_layers, batch_first=True)\n",
    "        elif cell_type == 'GRU':\n",
    "            self.rnn = nn.GRU(num_hidden, num_hidden, num_layers, batch_first=True)\n",
    "        elif cell_type == 'RNN':\n",
    "            self.rnn = nn.RNN(num_hidden, num_hidden, num_layers, batch_first=True)\n",
    "        else:\n",
    "            print('Invalid cell type, using LSTM')\n",
    "            self.rnn = nn.GRU(num_hidden, num_hidden, num_layers, batch_first=True)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(num_hidden, num_hidden),\n",
    "            nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, x, T):\n",
    "            \"\"\"\n",
    "            Parametri:\n",
    "              x: (batch, seq_len, num_hidden)\n",
    "              T : (Sequence Lengths)\n",
    "            Restituisce:\n",
    "              output: (batch, seq_len, num_hidden)\n",
    "              hidden: (num_layers, batch, num_hidden)\n",
    "            \"\"\"\n",
    "            packed = pack_padded_sequence(x, T, batch_first=True, enforce_sorted=False)\n",
    "            output, hidden = self.rnn(packed)\n",
    "            output, _ = pad_packed_sequence(output, batch_first=True, total_length=self.max_seq)\n",
    "            S = self.fc(output)\n",
    "            \n",
    "            return S # retrn the refined embeddings\n",
    "        \n",
    "\n",
    "num_layers_supervisor = 1\n",
    "hidden_dim_supervisor = 100\n",
    "cell_type_supervisor = 'GRU'\n",
    "\n",
    "supervisor = Supervisor(num_hidden = hidden_dim_supervisor,\n",
    "                        num_layers = num_layers_supervisor,\n",
    "                        max_seq = max_seq_len,\n",
    "                        cell_type = cell_type_supervisor)\n",
    "supervisor.apply(weights_init)\n",
    "supervisor.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedder(\n",
       "  (rnn): GRU(5, 100, batch_first=True)\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "class Embedder(nn.Module):\n",
    "    def __init__(self, inpt_dim, hidden_dim, num_layers,max_seq, cell_type = 'GRU'):\n",
    "        super(Embedder,self).__init__()\n",
    "        self.max_seq = max_seq\n",
    "        if cell_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(inpt_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        elif cell_type == 'GRU':\n",
    "            self.rnn = nn.GRU(inpt_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        elif cell_type == 'RNN':\n",
    "            self.rnn = nn.RNN(inpt_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        else:\n",
    "            print('Invalid cell type, using LSTM')\n",
    "            self.rnn = nn.GRU(inpt_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Sigmoid())\n",
    "    def forward(self, x, T):\n",
    "        \"\"\"\n",
    "        Parametri:\n",
    "          x: (batch, seq_len, inpt_dim)\n",
    "          T : (Sequence Lengths)\n",
    "        Restituisce:\n",
    "          output: (batch, seq_len, hidden_dim)\n",
    "          hidden: (num_layers, batch, hidden_dim)\n",
    "        \"\"\"\n",
    "        packed = pack_padded_sequence(x, T, batch_first=True, enforce_sorted=False)\n",
    "        output, hidden = self.rnn(packed)\n",
    "        output, _ = pad_packed_sequence(output, batch_first=True, total_length=self.max_seq)\n",
    "        H = self.fc(output)\n",
    "        \n",
    "        return H # retrn the embeddings\n",
    "\n",
    "num_layers_embedder = 1\n",
    "hidden_dim_embedder = 100\n",
    "cell_type_embedder = 'GRU'\n",
    "\n",
    "embedder = Embedder(inpt_dim = input_dim,\n",
    "                    hidden_dim = hidden_dim_embedder,\n",
    "                    num_layers = num_layers_embedder,\n",
    "                    max_seq = max_seq_len,\n",
    "                    cell_type = cell_type_embedder)\n",
    "embedder.apply(weights_init)\n",
    "embedder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Recovery(\n",
       "  (rnn): GRU(100, 100, batch_first=True)\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=5, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class Recovery(nn.Module):\n",
    "    def __init__(self, hidden_dim, feature_dim, num_layers,max_seq, cell_type = 'GRU'):\n",
    "        super(Recovery,self).__init__()\n",
    "        self.max_seq = max_seq\n",
    "        if cell_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        elif cell_type == 'GRU':\n",
    "            self.rnn = nn.GRU(hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        elif cell_type == 'RNN':\n",
    "            self.rnn = nn.RNN(hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        else:\n",
    "            print('Invalid cell type, using LSTM')\n",
    "            self.rnn = nn.GRU(hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, feature_dim),\n",
    "            nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, x, T):\n",
    "        \n",
    "        packed = pack_padded_sequence(x, T, batch_first=True, enforce_sorted=False)\n",
    "        output, hidden = self.rnn(packed)\n",
    "        output, _ = pad_packed_sequence(output, batch_first=True,total_length=self.max_seq)\n",
    "        X_tilde = self.fc(output)\n",
    "\n",
    "        return X_tilde # retrn the original data reconstruction\n",
    "    \n",
    "hidden_dim_recovery = 100\n",
    "num_layers_recovery = 1\n",
    "cell_type_recovery = 'GRU'\n",
    "\n",
    "recovery = Recovery(hidden_dim = hidden_dim_recovery,\n",
    "                    feature_dim = input_dim,\n",
    "                    num_layers = num_layers_recovery,\n",
    "                    max_seq = max_seq_len,\n",
    "                    cell_type = cell_type_recovery)\n",
    "recovery.apply(weights_init)\n",
    "recovery.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adesso la fase di training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_costs = []\n",
    "emb_rec_costs = []\n",
    "PRE_TRAIN_EPOCHS = 100\n",
    "\n",
    "for epoch in range(PRE_TRAIN_EPOCHS):\n",
    "    # pre-training of the embedder and recovery\n",
    "    for i, (real_samples, seq_lens) in enumerate(train_loader): \n",
    "        \n",
    "        embedder.train()\n",
    "        recovery.train()\n",
    "\n",
    "        real_samples = real_samples.to(device) # Send data to GPU\n",
    "        seq_lens = seq_lens.to(device) # Send sequence lengths to GPU\n",
    "\n",
    "        real_labels = torch.ones(real_samples.size(0), 1).to(device)\n",
    "        fake_labels = torch.zeros(real_samples.size(0), 1).to(device)\n",
    "\n",
    "        # Train the embedder\n",
    "        optimizer_E.zero_grad() # Reset gradients\n",
    "        \n",
    "        # create latent representation\n",
    "        H = embedder(real_samples, seq_lens) \n",
    "        # refine latent representation\n",
    "        X_tilde = recovery(H,seq_lens)\n",
    "        \n",
    "        # Calculate embedder loss\n",
    "        E_loss_T0 = mse_loss(X_tilde, real_samples) # reconstruction loss\n",
    "        E_loss0 = 10*torch.sqrt(E_loss_T0)\n",
    "        E_loss0.backward()\n",
    "\n",
    "        # Update embedder and recovery parameters\n",
    "        optimizer_E.step()\n",
    "        \n",
    "        emb_costs.append(E_loss0.item())\n",
    "\n",
    "        if not i % 100: \n",
    "            print ('Epoch: %03d/%03d | Batch %03d/%03d | Reconstruction Loss: %.4f' \n",
    "                    %(epoch+1, EPOCHS, i, \n",
    "                        len(train_loader), E_loss0))\n",
    "\n",
    "    # supervised training of the embedder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding training using the supervisor to have insight in the temporal dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/100 | Batch 000/002 | Reconstruction Loss: 8.7994\n",
      "Epoch: 002/100 | Batch 000/002 | Reconstruction Loss: 8.9522\n",
      "Epoch: 003/100 | Batch 000/002 | Reconstruction Loss: 8.8981\n",
      "Epoch: 004/100 | Batch 000/002 | Reconstruction Loss: 8.8690\n",
      "Epoch: 005/100 | Batch 000/002 | Reconstruction Loss: 8.8483\n",
      "Epoch: 006/100 | Batch 000/002 | Reconstruction Loss: 8.8708\n",
      "Epoch: 007/100 | Batch 000/002 | Reconstruction Loss: 8.8509\n",
      "Epoch: 008/100 | Batch 000/002 | Reconstruction Loss: 8.8471\n",
      "Epoch: 009/100 | Batch 000/002 | Reconstruction Loss: 8.9242\n",
      "Epoch: 010/100 | Batch 000/002 | Reconstruction Loss: 8.9138\n",
      "Epoch: 011/100 | Batch 000/002 | Reconstruction Loss: 8.9217\n",
      "Epoch: 012/100 | Batch 000/002 | Reconstruction Loss: 8.9187\n",
      "Epoch: 013/100 | Batch 000/002 | Reconstruction Loss: 8.8943\n",
      "Epoch: 014/100 | Batch 000/002 | Reconstruction Loss: 8.9210\n",
      "Epoch: 015/100 | Batch 000/002 | Reconstruction Loss: 8.8206\n",
      "Epoch: 016/100 | Batch 000/002 | Reconstruction Loss: 8.9198\n",
      "Epoch: 017/100 | Batch 000/002 | Reconstruction Loss: 8.9459\n",
      "Epoch: 018/100 | Batch 000/002 | Reconstruction Loss: 8.8774\n",
      "Epoch: 019/100 | Batch 000/002 | Reconstruction Loss: 8.8897\n",
      "Epoch: 020/100 | Batch 000/002 | Reconstruction Loss: 8.8747\n",
      "Epoch: 021/100 | Batch 000/002 | Reconstruction Loss: 8.9488\n",
      "Epoch: 022/100 | Batch 000/002 | Reconstruction Loss: 8.9313\n",
      "Epoch: 023/100 | Batch 000/002 | Reconstruction Loss: 8.9167\n",
      "Epoch: 024/100 | Batch 000/002 | Reconstruction Loss: 8.8302\n",
      "Epoch: 025/100 | Batch 000/002 | Reconstruction Loss: 8.8599\n",
      "Epoch: 026/100 | Batch 000/002 | Reconstruction Loss: 8.9030\n",
      "Epoch: 027/100 | Batch 000/002 | Reconstruction Loss: 8.9295\n",
      "Epoch: 028/100 | Batch 000/002 | Reconstruction Loss: 8.9046\n",
      "Epoch: 029/100 | Batch 000/002 | Reconstruction Loss: 8.8674\n",
      "Epoch: 030/100 | Batch 000/002 | Reconstruction Loss: 8.8576\n",
      "Epoch: 031/100 | Batch 000/002 | Reconstruction Loss: 8.7975\n",
      "Epoch: 032/100 | Batch 000/002 | Reconstruction Loss: 8.8487\n",
      "Epoch: 033/100 | Batch 000/002 | Reconstruction Loss: 8.8710\n",
      "Epoch: 034/100 | Batch 000/002 | Reconstruction Loss: 8.8726\n",
      "Epoch: 035/100 | Batch 000/002 | Reconstruction Loss: 8.8209\n",
      "Epoch: 036/100 | Batch 000/002 | Reconstruction Loss: 8.9313\n",
      "Epoch: 037/100 | Batch 000/002 | Reconstruction Loss: 8.9230\n",
      "Epoch: 038/100 | Batch 000/002 | Reconstruction Loss: 8.9347\n",
      "Epoch: 039/100 | Batch 000/002 | Reconstruction Loss: 8.8428\n",
      "Epoch: 040/100 | Batch 000/002 | Reconstruction Loss: 8.8997\n",
      "Epoch: 041/100 | Batch 000/002 | Reconstruction Loss: 8.8845\n",
      "Epoch: 042/100 | Batch 000/002 | Reconstruction Loss: 8.8913\n",
      "Epoch: 043/100 | Batch 000/002 | Reconstruction Loss: 8.8766\n",
      "Epoch: 044/100 | Batch 000/002 | Reconstruction Loss: 8.8772\n",
      "Epoch: 045/100 | Batch 000/002 | Reconstruction Loss: 8.8777\n",
      "Epoch: 046/100 | Batch 000/002 | Reconstruction Loss: 8.8592\n",
      "Epoch: 047/100 | Batch 000/002 | Reconstruction Loss: 8.7722\n",
      "Epoch: 048/100 | Batch 000/002 | Reconstruction Loss: 8.8814\n",
      "Epoch: 049/100 | Batch 000/002 | Reconstruction Loss: 8.8863\n",
      "Epoch: 050/100 | Batch 000/002 | Reconstruction Loss: 8.9172\n",
      "Epoch: 051/100 | Batch 000/002 | Reconstruction Loss: 8.9492\n",
      "Epoch: 052/100 | Batch 000/002 | Reconstruction Loss: 8.9189\n",
      "Epoch: 053/100 | Batch 000/002 | Reconstruction Loss: 8.8074\n",
      "Epoch: 054/100 | Batch 000/002 | Reconstruction Loss: 8.9381\n",
      "Epoch: 055/100 | Batch 000/002 | Reconstruction Loss: 8.9133\n",
      "Epoch: 056/100 | Batch 000/002 | Reconstruction Loss: 8.8997\n",
      "Epoch: 057/100 | Batch 000/002 | Reconstruction Loss: 8.9052\n",
      "Epoch: 058/100 | Batch 000/002 | Reconstruction Loss: 8.8828\n",
      "Epoch: 059/100 | Batch 000/002 | Reconstruction Loss: 8.9055\n",
      "Epoch: 060/100 | Batch 000/002 | Reconstruction Loss: 8.9111\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m H \u001b[38;5;241m=\u001b[39m embedder(real_samples,seq_lens)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# refine latent representation\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m H_hat_Supervise \u001b[38;5;241m=\u001b[39m \u001b[43msupervisor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43mseq_lens\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# reconstruct data from latent representation\u001b[39;00m\n\u001b[1;32m     15\u001b[0m X_tilde \u001b[38;5;241m=\u001b[39m recovery(H,seq_lens) \n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[99], line 38\u001b[0m, in \u001b[0;36mSupervisor.forward\u001b[0;34m(self, x, T)\u001b[0m\n\u001b[1;32m     36\u001b[0m output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(packed)\n\u001b[1;32m     37\u001b[0m output, _ \u001b[38;5;241m=\u001b[39m pad_packed_sequence(output, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 38\u001b[0m S \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m S\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/activation.py:327\u001b[0m, in \u001b[0;36mSigmoid.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(PRE_TRAIN_EPOCHS):\n",
    "    for i, (real_samples, seq_lens) in enumerate(train_loader): \n",
    "        \n",
    "        real_samples = real_samples.to(device)\n",
    "        seq_lens = seq_lens.to(device)\n",
    "\n",
    "        embedder.train()\n",
    "        recovery.train()\n",
    "\n",
    "        # create latent representation\n",
    "        H = embedder(real_samples,seq_lens)\n",
    "        # refine latent representation\n",
    "        H_hat_Supervise = supervisor(H,seq_lens).detach()\n",
    "        # reconstruct data from latent representation\n",
    "        X_tilde = recovery(H,seq_lens) \n",
    "        \n",
    "        # Calculate embedder loss\n",
    "        # supervised loss\n",
    "        G_loss_S = mse_loss(H[:, 1:, :], H_hat_Supervise[:, :-1, :])\n",
    "        # reconstruction loss\n",
    "        E_loss_T0 = mse_loss(X_tilde, real_samples)\n",
    "\n",
    "        E_loss0 = 10*torch.sqrt(E_loss_T0)\n",
    "        E_loss = E_loss0 + 0.1*G_loss_S # total embedder loss\n",
    "\n",
    "        # Backpropagate and update weights\n",
    "        E_loss.backward()\n",
    "\n",
    "        optimizer_E.step()\n",
    "        emb_rec_costs.append(E_loss.item())\n",
    "\n",
    "        if not i % 100: \n",
    "            print ('Epoch: %03d/%03d | Batch %03d/%03d | Reconstruction Loss: %.4f' \n",
    "                    %(epoch+1, EPOCHS, i, \n",
    "                        len(train_loader), E_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joint Training of the supervisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 002/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 003/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 004/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 005/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 006/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 007/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 008/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 009/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 010/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 011/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 012/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 013/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 014/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 015/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 016/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 017/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 018/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 019/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 020/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 021/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 022/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 023/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 024/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 025/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 026/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 027/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 028/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 029/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 030/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 031/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 032/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 033/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 034/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 035/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 036/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 037/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 038/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 039/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 040/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 041/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 042/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 043/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 044/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 045/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 046/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 047/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 048/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 049/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 050/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 051/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 052/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 053/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 054/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 055/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 056/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 057/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 058/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 059/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 060/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 061/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 062/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 063/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 064/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 065/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 066/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 067/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 068/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 069/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 070/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 071/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 072/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 073/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 074/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 075/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 076/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 077/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 078/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 079/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 080/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 081/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 082/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 083/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 084/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 085/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 086/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 087/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 088/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 089/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 090/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 091/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 092/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 093/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 094/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 095/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 096/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 097/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 098/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 099/100 | Batch 000/002 | Supervisor Loss: 0.0072\n",
      "Epoch: 100/100 | Batch 000/002 | Supervisor Loss: 0.0072\n"
     ]
    }
   ],
   "source": [
    "supervisor_costs = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Step 2: Generator, supervised only\n",
    "    for i, (real_samples, seq_lens) in enumerate(train_loader):  # loop over batches\n",
    "\n",
    "        real_samples = real_samples.to(device)\n",
    "        seq_lens = seq_lens.to(device)\n",
    "\n",
    "        generator.train()\n",
    "        supervisor.train()\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # generate real data embeddings\n",
    "        H = embedder(real_samples,seq_lens).detach()\n",
    "        H_hat_Supervise = supervisor(H, seq_lens)\n",
    "\n",
    "        G_loss_S = mse_loss(H[:, 1:, :], H_hat_Supervise[:, :-1, :]) # supervised loss\n",
    "        G_loss_S.backward()\n",
    "        \n",
    "        optimizer_G.step()\n",
    "\n",
    "        supervisor_costs.append(G_loss_S.item())\n",
    "\n",
    "        if not i % 100: \n",
    "            print ('Epoch: %03d/%03d | Batch %03d/%03d | Supervisor Loss: %.4f' \n",
    "                    %(epoch+1, EPOCHS, i, \n",
    "                        len(train_loader), G_loss_S))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Congiunto dell'architettura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/100 | Batch 000/002 | Discriminator Loss: 1.4667\n",
      "Generator Loss: 191.1326 | Embedder Loss: 0.7717 | Supervisor Loss: 0.0074\n",
      "Epoch: 002/100 | Batch 000/002 | Discriminator Loss: 1.4669\n",
      "Generator Loss: 192.8333 | Embedder Loss: 0.7763 | Supervisor Loss: 0.0075\n",
      "Epoch: 003/100 | Batch 000/002 | Discriminator Loss: 1.4671\n",
      "Generator Loss: 191.8645 | Embedder Loss: 0.7674 | Supervisor Loss: 0.0074\n",
      "Epoch: 004/100 | Batch 000/002 | Discriminator Loss: 1.4671\n",
      "Generator Loss: 192.2735 | Embedder Loss: 0.7801 | Supervisor Loss: 0.0075\n",
      "Epoch: 005/100 | Batch 000/002 | Discriminator Loss: 1.4670\n",
      "Generator Loss: 192.1727 | Embedder Loss: 0.7781 | Supervisor Loss: 0.0074\n",
      "Epoch: 006/100 | Batch 000/002 | Discriminator Loss: 1.4670\n",
      "Generator Loss: 193.0482 | Embedder Loss: 0.7872 | Supervisor Loss: 0.0075\n",
      "Epoch: 007/100 | Batch 000/002 | Discriminator Loss: 1.4671\n",
      "Generator Loss: 191.8546 | Embedder Loss: 0.7764 | Supervisor Loss: 0.0074\n",
      "Epoch: 008/100 | Batch 000/002 | Discriminator Loss: 1.4674\n",
      "Generator Loss: 193.3307 | Embedder Loss: 0.7929 | Supervisor Loss: 0.0075\n",
      "Epoch: 009/100 | Batch 000/002 | Discriminator Loss: 1.4669\n",
      "Generator Loss: 192.0814 | Embedder Loss: 0.7790 | Supervisor Loss: 0.0074\n",
      "Epoch: 010/100 | Batch 000/002 | Discriminator Loss: 1.4671\n",
      "Generator Loss: 191.7143 | Embedder Loss: 0.7790 | Supervisor Loss: 0.0074\n",
      "Epoch: 011/100 | Batch 000/002 | Discriminator Loss: 1.4669\n",
      "Generator Loss: 191.4355 | Embedder Loss: 0.7730 | Supervisor Loss: 0.0075\n",
      "Epoch: 012/100 | Batch 000/002 | Discriminator Loss: 1.4671\n",
      "Generator Loss: 192.6917 | Embedder Loss: 0.7927 | Supervisor Loss: 0.0075\n",
      "Epoch: 013/100 | Batch 000/002 | Discriminator Loss: 1.4672\n",
      "Generator Loss: 192.0928 | Embedder Loss: 0.7732 | Supervisor Loss: 0.0074\n",
      "Epoch: 014/100 | Batch 000/002 | Discriminator Loss: 1.4670\n",
      "Generator Loss: 192.1780 | Embedder Loss: 0.7796 | Supervisor Loss: 0.0075\n",
      "Epoch: 015/100 | Batch 000/002 | Discriminator Loss: 1.4673\n",
      "Generator Loss: 193.5895 | Embedder Loss: 0.7905 | Supervisor Loss: 0.0075\n",
      "Epoch: 016/100 | Batch 000/002 | Discriminator Loss: 1.4670\n",
      "Generator Loss: 192.1799 | Embedder Loss: 0.7810 | Supervisor Loss: 0.0074\n",
      "Epoch: 017/100 | Batch 000/002 | Discriminator Loss: 1.4669\n",
      "Generator Loss: 192.1610 | Embedder Loss: 0.7807 | Supervisor Loss: 0.0074\n",
      "Epoch: 018/100 | Batch 000/002 | Discriminator Loss: 1.4667\n",
      "Generator Loss: 191.6318 | Embedder Loss: 0.7769 | Supervisor Loss: 0.0074\n",
      "Epoch: 019/100 | Batch 000/002 | Discriminator Loss: 1.4674\n",
      "Generator Loss: 192.6415 | Embedder Loss: 0.7872 | Supervisor Loss: 0.0075\n",
      "Epoch: 020/100 | Batch 000/002 | Discriminator Loss: 1.4676\n",
      "Generator Loss: 191.6589 | Embedder Loss: 0.7661 | Supervisor Loss: 0.0074\n",
      "Epoch: 021/100 | Batch 000/002 | Discriminator Loss: 1.4668\n",
      "Generator Loss: 191.9645 | Embedder Loss: 0.7693 | Supervisor Loss: 0.0074\n",
      "Epoch: 022/100 | Batch 000/002 | Discriminator Loss: 1.4674\n",
      "Generator Loss: 192.0111 | Embedder Loss: 0.7852 | Supervisor Loss: 0.0074\n",
      "Epoch: 023/100 | Batch 000/002 | Discriminator Loss: 1.4670\n",
      "Generator Loss: 192.4794 | Embedder Loss: 0.7790 | Supervisor Loss: 0.0075\n",
      "Epoch: 024/100 | Batch 000/002 | Discriminator Loss: 1.4671\n",
      "Generator Loss: 192.0066 | Embedder Loss: 0.7771 | Supervisor Loss: 0.0075\n",
      "Epoch: 025/100 | Batch 000/002 | Discriminator Loss: 1.4674\n",
      "Generator Loss: 192.6756 | Embedder Loss: 0.7822 | Supervisor Loss: 0.0075\n",
      "Epoch: 026/100 | Batch 000/002 | Discriminator Loss: 1.4674\n",
      "Generator Loss: 192.2089 | Embedder Loss: 0.7820 | Supervisor Loss: 0.0075\n",
      "Epoch: 027/100 | Batch 000/002 | Discriminator Loss: 1.4676\n",
      "Generator Loss: 192.3729 | Embedder Loss: 0.7883 | Supervisor Loss: 0.0075\n",
      "Epoch: 028/100 | Batch 000/002 | Discriminator Loss: 1.4672\n",
      "Generator Loss: 193.0793 | Embedder Loss: 0.7937 | Supervisor Loss: 0.0075\n",
      "Epoch: 029/100 | Batch 000/002 | Discriminator Loss: 1.4673\n",
      "Generator Loss: 193.0141 | Embedder Loss: 0.7851 | Supervisor Loss: 0.0074\n",
      "Epoch: 030/100 | Batch 000/002 | Discriminator Loss: 1.4671\n",
      "Generator Loss: 192.0426 | Embedder Loss: 0.7771 | Supervisor Loss: 0.0074\n",
      "Epoch: 031/100 | Batch 000/002 | Discriminator Loss: 1.4673\n",
      "Generator Loss: 192.1199 | Embedder Loss: 0.7835 | Supervisor Loss: 0.0075\n",
      "Epoch: 032/100 | Batch 000/002 | Discriminator Loss: 1.4667\n",
      "Generator Loss: 191.1729 | Embedder Loss: 0.7722 | Supervisor Loss: 0.0074\n",
      "Epoch: 033/100 | Batch 000/002 | Discriminator Loss: 1.4670\n",
      "Generator Loss: 192.0531 | Embedder Loss: 0.7693 | Supervisor Loss: 0.0074\n",
      "Epoch: 034/100 | Batch 000/002 | Discriminator Loss: 1.4672\n",
      "Generator Loss: 192.1821 | Embedder Loss: 0.7820 | Supervisor Loss: 0.0074\n",
      "Epoch: 035/100 | Batch 000/002 | Discriminator Loss: 1.4673\n",
      "Generator Loss: 193.0559 | Embedder Loss: 0.7955 | Supervisor Loss: 0.0075\n",
      "Epoch: 036/100 | Batch 000/002 | Discriminator Loss: 1.4670\n",
      "Generator Loss: 192.8776 | Embedder Loss: 0.7936 | Supervisor Loss: 0.0075\n",
      "Epoch: 037/100 | Batch 000/002 | Discriminator Loss: 1.4672\n",
      "Generator Loss: 192.1527 | Embedder Loss: 0.7745 | Supervisor Loss: 0.0074\n",
      "Epoch: 038/100 | Batch 000/002 | Discriminator Loss: 1.4669\n",
      "Generator Loss: 192.0817 | Embedder Loss: 0.7816 | Supervisor Loss: 0.0075\n",
      "Epoch: 039/100 | Batch 000/002 | Discriminator Loss: 1.4667\n",
      "Generator Loss: 192.0567 | Embedder Loss: 0.7738 | Supervisor Loss: 0.0074\n",
      "Epoch: 040/100 | Batch 000/002 | Discriminator Loss: 1.4668\n",
      "Generator Loss: 192.9959 | Embedder Loss: 0.7949 | Supervisor Loss: 0.0075\n",
      "Epoch: 041/100 | Batch 000/002 | Discriminator Loss: 1.4671\n",
      "Generator Loss: 192.0822 | Embedder Loss: 0.7754 | Supervisor Loss: 0.0074\n",
      "Epoch: 042/100 | Batch 000/002 | Discriminator Loss: 1.4671\n",
      "Generator Loss: 191.9367 | Embedder Loss: 0.7754 | Supervisor Loss: 0.0075\n",
      "Epoch: 043/100 | Batch 000/002 | Discriminator Loss: 1.4671\n",
      "Generator Loss: 191.7183 | Embedder Loss: 0.7808 | Supervisor Loss: 0.0075\n",
      "Epoch: 044/100 | Batch 000/002 | Discriminator Loss: 1.4672\n",
      "Generator Loss: 191.9203 | Embedder Loss: 0.7800 | Supervisor Loss: 0.0074\n",
      "Epoch: 045/100 | Batch 000/002 | Discriminator Loss: 1.4670\n",
      "Generator Loss: 191.6815 | Embedder Loss: 0.7719 | Supervisor Loss: 0.0074\n",
      "Epoch: 046/100 | Batch 000/002 | Discriminator Loss: 1.4671\n",
      "Generator Loss: 192.3235 | Embedder Loss: 0.7810 | Supervisor Loss: 0.0075\n",
      "Epoch: 047/100 | Batch 000/002 | Discriminator Loss: 1.4670\n",
      "Generator Loss: 192.1888 | Embedder Loss: 0.7812 | Supervisor Loss: 0.0075\n",
      "Epoch: 048/100 | Batch 000/002 | Discriminator Loss: 1.4671\n",
      "Generator Loss: 192.3212 | Embedder Loss: 0.7786 | Supervisor Loss: 0.0075\n",
      "Epoch: 049/100 | Batch 000/002 | Discriminator Loss: 1.4672\n",
      "Generator Loss: 192.3529 | Embedder Loss: 0.7712 | Supervisor Loss: 0.0074\n",
      "Epoch: 050/100 | Batch 000/002 | Discriminator Loss: 1.4671\n",
      "Generator Loss: 191.8215 | Embedder Loss: 0.7707 | Supervisor Loss: 0.0074\n",
      "Epoch: 051/100 | Batch 000/002 | Discriminator Loss: 1.4671\n",
      "Generator Loss: 191.4991 | Embedder Loss: 0.7837 | Supervisor Loss: 0.0074\n",
      "Epoch: 052/100 | Batch 000/002 | Discriminator Loss: 1.4668\n",
      "Generator Loss: 192.6352 | Embedder Loss: 0.7902 | Supervisor Loss: 0.0074\n",
      "Epoch: 053/100 | Batch 000/002 | Discriminator Loss: 1.4672\n",
      "Generator Loss: 192.6497 | Embedder Loss: 0.7880 | Supervisor Loss: 0.0075\n",
      "Epoch: 054/100 | Batch 000/002 | Discriminator Loss: 1.4676\n",
      "Generator Loss: 192.3604 | Embedder Loss: 0.7872 | Supervisor Loss: 0.0075\n",
      "Epoch: 055/100 | Batch 000/002 | Discriminator Loss: 1.4673\n",
      "Generator Loss: 193.2873 | Embedder Loss: 0.7972 | Supervisor Loss: 0.0075\n",
      "Epoch: 056/100 | Batch 000/002 | Discriminator Loss: 1.4673\n",
      "Generator Loss: 193.0827 | Embedder Loss: 0.7833 | Supervisor Loss: 0.0074\n",
      "Epoch: 057/100 | Batch 000/002 | Discriminator Loss: 1.4668\n",
      "Generator Loss: 192.3219 | Embedder Loss: 0.7848 | Supervisor Loss: 0.0075\n",
      "Epoch: 058/100 | Batch 000/002 | Discriminator Loss: 1.4673\n",
      "Generator Loss: 191.7120 | Embedder Loss: 0.7710 | Supervisor Loss: 0.0074\n",
      "Epoch: 059/100 | Batch 000/002 | Discriminator Loss: 1.4670\n",
      "Generator Loss: 192.3337 | Embedder Loss: 0.7732 | Supervisor Loss: 0.0075\n",
      "Epoch: 060/100 | Batch 000/002 | Discriminator Loss: 1.4672\n",
      "Generator Loss: 192.1576 | Embedder Loss: 0.7835 | Supervisor Loss: 0.0075\n",
      "Epoch: 061/100 | Batch 000/002 | Discriminator Loss: 1.4667\n",
      "Generator Loss: 190.9582 | Embedder Loss: 0.7729 | Supervisor Loss: 0.0074\n",
      "Epoch: 062/100 | Batch 000/002 | Discriminator Loss: 1.4669\n",
      "Generator Loss: 191.6357 | Embedder Loss: 0.7705 | Supervisor Loss: 0.0074\n",
      "Epoch: 063/100 | Batch 000/002 | Discriminator Loss: 1.4671\n",
      "Generator Loss: 193.0416 | Embedder Loss: 0.7882 | Supervisor Loss: 0.0075\n",
      "Epoch: 064/100 | Batch 000/002 | Discriminator Loss: 1.4673\n",
      "Generator Loss: 191.8285 | Embedder Loss: 0.7790 | Supervisor Loss: 0.0074\n",
      "Epoch: 065/100 | Batch 000/002 | Discriminator Loss: 1.4669\n",
      "Generator Loss: 192.1967 | Embedder Loss: 0.7827 | Supervisor Loss: 0.0074\n",
      "Epoch: 066/100 | Batch 000/002 | Discriminator Loss: 1.4668\n",
      "Generator Loss: 192.4676 | Embedder Loss: 0.7886 | Supervisor Loss: 0.0075\n",
      "Epoch: 067/100 | Batch 000/002 | Discriminator Loss: 1.4670\n",
      "Generator Loss: 190.9836 | Embedder Loss: 0.7620 | Supervisor Loss: 0.0074\n",
      "Epoch: 068/100 | Batch 000/002 | Discriminator Loss: 1.4672\n",
      "Generator Loss: 191.9495 | Embedder Loss: 0.7673 | Supervisor Loss: 0.0074\n",
      "Epoch: 069/100 | Batch 000/002 | Discriminator Loss: 1.4673\n",
      "Generator Loss: 192.4934 | Embedder Loss: 0.7851 | Supervisor Loss: 0.0075\n",
      "Epoch: 070/100 | Batch 000/002 | Discriminator Loss: 1.4670\n",
      "Generator Loss: 192.3830 | Embedder Loss: 0.7848 | Supervisor Loss: 0.0075\n",
      "Epoch: 071/100 | Batch 000/002 | Discriminator Loss: 1.4671\n",
      "Generator Loss: 192.2148 | Embedder Loss: 0.7764 | Supervisor Loss: 0.0074\n",
      "Epoch: 072/100 | Batch 000/002 | Discriminator Loss: 1.4671\n",
      "Generator Loss: 191.9069 | Embedder Loss: 0.7682 | Supervisor Loss: 0.0074\n",
      "Epoch: 073/100 | Batch 000/002 | Discriminator Loss: 1.4670\n",
      "Generator Loss: 191.9532 | Embedder Loss: 0.7839 | Supervisor Loss: 0.0074\n",
      "Epoch: 074/100 | Batch 000/002 | Discriminator Loss: 1.4676\n",
      "Generator Loss: 192.2741 | Embedder Loss: 0.7928 | Supervisor Loss: 0.0075\n",
      "Epoch: 075/100 | Batch 000/002 | Discriminator Loss: 1.4668\n",
      "Generator Loss: 192.2796 | Embedder Loss: 0.7787 | Supervisor Loss: 0.0074\n",
      "Epoch: 076/100 | Batch 000/002 | Discriminator Loss: 1.4668\n",
      "Generator Loss: 191.0210 | Embedder Loss: 0.7712 | Supervisor Loss: 0.0074\n",
      "Epoch: 077/100 | Batch 000/002 | Discriminator Loss: 1.4669\n",
      "Generator Loss: 191.9715 | Embedder Loss: 0.7766 | Supervisor Loss: 0.0074\n",
      "Epoch: 078/100 | Batch 000/002 | Discriminator Loss: 1.4675\n",
      "Generator Loss: 192.5179 | Embedder Loss: 0.7874 | Supervisor Loss: 0.0075\n",
      "Epoch: 079/100 | Batch 000/002 | Discriminator Loss: 1.4670\n",
      "Generator Loss: 192.6170 | Embedder Loss: 0.7901 | Supervisor Loss: 0.0075\n",
      "Epoch: 080/100 | Batch 000/002 | Discriminator Loss: 1.4671\n",
      "Generator Loss: 191.9627 | Embedder Loss: 0.7761 | Supervisor Loss: 0.0074\n",
      "Epoch: 081/100 | Batch 000/002 | Discriminator Loss: 1.4670\n",
      "Generator Loss: 192.4617 | Embedder Loss: 0.7727 | Supervisor Loss: 0.0074\n",
      "Epoch: 082/100 | Batch 000/002 | Discriminator Loss: 1.4669\n",
      "Generator Loss: 191.5247 | Embedder Loss: 0.7754 | Supervisor Loss: 0.0074\n",
      "Epoch: 083/100 | Batch 000/002 | Discriminator Loss: 1.4670\n",
      "Generator Loss: 191.2767 | Embedder Loss: 0.7713 | Supervisor Loss: 0.0074\n",
      "Epoch: 084/100 | Batch 000/002 | Discriminator Loss: 1.4669\n",
      "Generator Loss: 191.6559 | Embedder Loss: 0.7666 | Supervisor Loss: 0.0074\n",
      "Epoch: 085/100 | Batch 000/002 | Discriminator Loss: 1.4670\n",
      "Generator Loss: 191.7329 | Embedder Loss: 0.7768 | Supervisor Loss: 0.0074\n",
      "Epoch: 086/100 | Batch 000/002 | Discriminator Loss: 1.4671\n",
      "Generator Loss: 192.1940 | Embedder Loss: 0.7821 | Supervisor Loss: 0.0075\n",
      "Epoch: 087/100 | Batch 000/002 | Discriminator Loss: 1.4669\n",
      "Generator Loss: 192.7474 | Embedder Loss: 0.7759 | Supervisor Loss: 0.0075\n",
      "Epoch: 088/100 | Batch 000/002 | Discriminator Loss: 1.4673\n",
      "Generator Loss: 191.6017 | Embedder Loss: 0.7762 | Supervisor Loss: 0.0075\n",
      "Epoch: 089/100 | Batch 000/002 | Discriminator Loss: 1.4672\n",
      "Generator Loss: 191.4430 | Embedder Loss: 0.7720 | Supervisor Loss: 0.0075\n",
      "Epoch: 090/100 | Batch 000/002 | Discriminator Loss: 1.4671\n",
      "Generator Loss: 192.2274 | Embedder Loss: 0.7797 | Supervisor Loss: 0.0074\n",
      "Epoch: 091/100 | Batch 000/002 | Discriminator Loss: 1.4673\n",
      "Generator Loss: 191.5012 | Embedder Loss: 0.7751 | Supervisor Loss: 0.0074\n",
      "Epoch: 092/100 | Batch 000/002 | Discriminator Loss: 1.4668\n",
      "Generator Loss: 192.1359 | Embedder Loss: 0.7853 | Supervisor Loss: 0.0075\n",
      "Epoch: 093/100 | Batch 000/002 | Discriminator Loss: 1.4670\n",
      "Generator Loss: 192.4453 | Embedder Loss: 0.7928 | Supervisor Loss: 0.0075\n",
      "Epoch: 094/100 | Batch 000/002 | Discriminator Loss: 1.4679\n",
      "Generator Loss: 193.2249 | Embedder Loss: 0.8007 | Supervisor Loss: 0.0075\n",
      "Epoch: 095/100 | Batch 000/002 | Discriminator Loss: 1.4672\n",
      "Generator Loss: 192.4391 | Embedder Loss: 0.7811 | Supervisor Loss: 0.0075\n",
      "Epoch: 096/100 | Batch 000/002 | Discriminator Loss: 1.4670\n",
      "Generator Loss: 191.6120 | Embedder Loss: 0.7744 | Supervisor Loss: 0.0074\n",
      "Epoch: 097/100 | Batch 000/002 | Discriminator Loss: 1.4672\n",
      "Generator Loss: 192.6063 | Embedder Loss: 0.7807 | Supervisor Loss: 0.0075\n",
      "Epoch: 098/100 | Batch 000/002 | Discriminator Loss: 1.4674\n",
      "Generator Loss: 192.1836 | Embedder Loss: 0.7837 | Supervisor Loss: 0.0075\n",
      "Epoch: 099/100 | Batch 000/002 | Discriminator Loss: 1.4671\n",
      "Generator Loss: 192.0833 | Embedder Loss: 0.7813 | Supervisor Loss: 0.0074\n",
      "Epoch: 100/100 | Batch 000/002 | Discriminator Loss: 1.4671\n",
      "Generator Loss: 192.2198 | Embedder Loss: 0.7751 | Supervisor Loss: 0.0074\n"
     ]
    }
   ],
   "source": [
    "generator_losses = []\n",
    "discriminator_losses = []\n",
    "emb_costs = []\n",
    "supervisor_costs = []\n",
    "for epoch in range(EPOCHS):\n",
    "    for i,(real_samples, seq_lens) in enumerate(train_loader):\n",
    "        \n",
    "        # discriminator network\n",
    "        discriminator.train()\n",
    "        # generator network\n",
    "        generator.train()\n",
    "        supervisor.train()\n",
    "        # embedding network \n",
    "        embedder.train()\n",
    "        recovery.train()\n",
    "\n",
    "        \n",
    "        real_samples = real_samples.to(device)\n",
    "        seq_lens = seq_lens.to(device)\n",
    "        # train the generator twice as much as the discriminator\n",
    "        for kk in range(2): \n",
    "            # Step 3.1: Generator training\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            \n",
    "            # sample random noise\n",
    "            Z_mb = random_generator(len(real_samples), hidden_dim_generator, seq_lens, seq_len)\n",
    "            H = embedder(real_samples,seq_lens).detach() \n",
    "            # generate fake data embeddings\n",
    "            E_hat = generator(Z_mb,seq_lens)\n",
    "            # refine fake data embeddings\n",
    "            H_hat = supervisor(E_hat,seq_lens)\n",
    "            # reconstruct fake data\n",
    "            X_hat = recovery(H_hat,seq_lens).detach()\n",
    "            # discriminator outputs\n",
    "            Y_fake = discriminator(H_hat, seq_lens).detach() # \n",
    "            Y_real = discriminator(H, seq_lens).detach()     \n",
    "            Y_fake_e = discriminator(E_hat, seq_lens).detach()\n",
    "            \n",
    "            # unsupervised loss\n",
    "            G_loss_U = bce_loss(torch.ones_like(Y_fake), Y_fake) # after supervisor\n",
    "            G_loss_U_e = bce_loss(torch.ones_like(Y_fake_e), Y_fake_e) # before supervisor\n",
    "            # supervised loss\n",
    "            # H_hat = supervisor(generator(Z_mb,seq_lens),seq_lens)\n",
    "            G_loss_S = mse_loss(H[:, 1:, :], H_hat[:, :-1, :]) # supervised loss\n",
    "            \n",
    "            G_loss_V1 = torch.mean(torch.abs(torch.sqrt(torch.var(real_samples, dim=0) + 1e-6) - torch.sqrt(torch.var(X_hat, dim=0) + 1e-6)))\n",
    "            G_loss_V2 = torch.mean(torch.abs(torch.mean(real_samples, dim=0) - torch.mean(X_hat, dim=0)))\n",
    "            G_loss_V = G_loss_V1 + G_loss_V2\n",
    "            \n",
    "            # Total generator loss\n",
    "            G_loss = G_loss_U + G_loss_U_e + 100*torch.sqrt(G_loss_S) + 100*G_loss_V \n",
    "            G_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            generator_losses.append(G_loss.item())\n",
    "            supervisor_costs.append(G_loss_S.item())\n",
    "\n",
    "\n",
    "            # step 3.2: embedder and recovery training\n",
    "            optimizer_E.zero_grad()\n",
    "            \n",
    "            X_tilde = recovery(H, seq_lens) \n",
    "            # reconstrucion loss\n",
    "            E_loss_T0 = mse_loss(X_tilde, real_samples)\n",
    "            E_loss_T0.backward()\n",
    "            optimizer_E.step()\n",
    "            emb_costs.append(E_loss_T0.item())\n",
    "        \n",
    "        # Step 4: Discriminator training\n",
    "        optimizer_D.zero_grad()  # Zero discriminator gradients\n",
    "        \n",
    "        # Get discriminator outputs for real data\n",
    "\n",
    "        H = embedder(real_samples, seq_lens).detach()  # Embed real data\n",
    "\n",
    "        E_hat = generator(Z_mb, seq_lens).detach()  # Generate fake data\n",
    "        H_hat = supervisor(E_hat, seq_lens).detach()  # Supervise fake data\n",
    "        X_hat = recovery(H_hat, seq_lens).detach()  # Recover fake data\n",
    "\n",
    "        Y_fake = discriminator(H_hat, seq_lens)  # Prediction on supervised fake data\n",
    "        Y_fake_e = discriminator(E_hat, seq_lens)  # Prediction on directly generated data\n",
    "        Y_real = discriminator(H, seq_lens)  # Discriminator prediction on real data\n",
    "        \n",
    "        # Calculate discriminator loss - wants to predict real=1, fake=0\n",
    "        D_loss_real = F.binary_cross_entropy(Y_real, torch.ones_like(Y_real))\n",
    "    \n",
    "        # For generated data, target is 0\n",
    "        D_loss_fake = F.binary_cross_entropy(Y_fake, torch.zeros_like(Y_fake))\n",
    "        D_loss_fake_e = F.binary_cross_entropy(Y_fake_e, torch.zeros_like(Y_fake_e))\n",
    "        \n",
    "        # Total loss: sum of components, with gamma balancing the loss_fake_e term\n",
    "        D_loss = D_loss_real + D_loss_fake + gamma * D_loss_fake_e\n",
    "    \n",
    "\n",
    "        # Backpropagate and update discriminator weights\n",
    "        D_loss.backward()\n",
    "        \n",
    "        optimizer_D.step()\n",
    "        discriminator_losses.append(D_loss.item())\n",
    "\n",
    "        if not i % 100:\n",
    "            print ('Epoch: %03d/%03d | Batch %03d/%03d | Discriminator Loss: %.4f' \n",
    "                    %(epoch+1, EPOCHS, i, \n",
    "                        len(train_loader), D_loss))\n",
    "            print ('Generator Loss: %.4f | Embedder Loss: %.4f | Supervisor Loss: %.4f'\n",
    "                    %(G_loss, E_loss_T0, G_loss_S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Crea una cartella con timestamp per organizzare meglio i salvataggi\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "save_dir = f\"./saved_models/timegan_{timestamp}\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Salva gli iperparametri per poter riprodurre il modello\n",
    "hyperparams = {\n",
    "    \"hidden_dim\": 100,  # Sostituisci con i tuoi valori effettivi\n",
    "    \"num_layers\": 1,\n",
    "    \"cell_type\": \"GRU\",\n",
    "    \"input_dim\": 5,\n",
    "    \"latent_dim\": 100,\n",
    "    \"gamma\": 0.1,\n",
    "    \"batch_size\": 64,\n",
    "    \"pre_train_epochs\": 100,\n",
    "    \"train_epochs\": 50  # Numero di epoche effettivamente completate\n",
    "}\n",
    "\n",
    "# Salva gli iperparametri\n",
    "with open(f\"{save_dir}/hyperparameters.json\", \"w\") as f:\n",
    "    json.dump(hyperparams, f, indent=4)\n",
    "\n",
    "# Salva il modello completo\n",
    "torch.save(model.state_dict(), f\"{save_dir}/timegan_full_model.pt\")\n",
    "\n",
    "# Salva i componenti individuali\n",
    "torch.save(generator.state_dict(), f\"{save_dir}/generator.pt\")\n",
    "torch.save(discriminator.state_dict(), f\"{save_dir}/discriminator.pt\")\n",
    "torch.save(supervisor.state_dict(), f\"{save_dir}/supervisor.pt\")\n",
    "torch.save(embedder.state_dict(), f\"{save_dir}/embedder.pt\")\n",
    "torch.save(recovery.state_dict(), f\"{save_dir}/recovery.pt\")\n",
    "\n",
    "# Salva anche gli stati degli ottimizzatori (utile per riprendere l'addestramento)\n",
    "optimizer_states = {\n",
    "    \"optimizer_G\": optimizer_G.state_dict(),\n",
    "    \"optimizer_D\": optimizer_D.state_dict(),\n",
    "    \"optimizer_E\": optimizer_E.state_dict(),\n",
    "}\n",
    "torch.save(optimizer_states, f\"{save_dir}/optimizer_states.pt\")\n",
    "\n",
    "# Salva la storia dell'addestramento\n",
    "training_history = {\n",
    "    \"generator_losses\": generator_losses,\n",
    "    \"discriminator_losses\": discriminator_losses,\n",
    "    \"embedding_losses\": emb_costs,\n",
    "    \"supervisor_losses\": supervisor_costs\n",
    "}\n",
    "torch.save(training_history, f\"{save_dir}/training_history.pt\")\n",
    "# Salva anche come numpy per facilità di analisi\n",
    "np.save(f\"{save_dir}/generator_losses.npy\", np.array(generator_losses))\n",
    "np.save(f\"{save_dir}/discriminator_losses.npy\", np.array(discriminator_losses))\n",
    "np.save(f\"{save_dir}/embedding_losses.npy\", np.array(emb_costs))\n",
    "np.save(f\"{save_dir}/supervisor_losses.npy\", np.array(supervisor_costs))\n",
    "\n",
    "# Salva un file README con informazioni sul modello\n",
    "with open(f\"{save_dir}/README.md\", \"w\") as f:\n",
    "    f.write(f\"# TimeGAN Model saved on {timestamp}\\n\\n\")\n",
    "    f.write(\"## Training Information\\n\")\n",
    "    f.write(f\"- Pre-training epochs: {hyperparams['pre_train_epochs']}\\n\")\n",
    "    f.write(f\"- Training epochs: {hyperparams['train_epochs']}\\n\")\n",
    "    f.write(f\"- Final generator loss: {generator_losses[-1] if generator_losses else 'N/A'}\\n\")\n",
    "    f.write(f\"- Final discriminator loss: {discriminator_losses[-1] if discriminator_losses else 'N/A'}\\n\")\n",
    "\n",
    "print(f\"Model and training history saved to {save_dir}\")\n",
    "print(f\"Use 'model = TimeGAN(**hyperparams); model.load_state_dict(torch.load(\\\"{save_dir}/timegan_full_model.pt\\\"))' to load\")\n",
    "\n",
    "# Save individual components for flexibility\n",
    "torch.save(generator.state_dict(), f\"{save_dir}/generator.pt\")\n",
    "torch.save(discriminator.state_dict(), f\"{save_dir}/discriminator.pt\")\n",
    "torch.save(supervisor.state_dict(), f\"{save_dir}/supervisor.pt\")\n",
    "torch.save(embedder.state_dict(), f\"{save_dir}/embedder.pt\")\n",
    "torch.save(recovery.state_dict(), f\"{save_dir}/recovery.pt\")\n",
    "\n",
    "# Save training history\n",
    "np.save(f\"{save_dir}/generator_losses.npy\", np.array(generator_losses))\n",
    "np.save(f\"{save_dir}/discriminator_losses.npy\", np.array(discriminator_losses))\n",
    "np.save(f\"{save_dir}/embedding_losses.npy\", np.array(emb_costs))\n",
    "np.save(f\"{save_dir}/supervisor_losses.npy\", np.array(supervisor_costs))\n",
    "\n",
    "print(f\"Model and training history saved to {save_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
